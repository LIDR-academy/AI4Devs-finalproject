# ‚ö° Performance & Scalability - Nura System

## ‚ö° Estrategia de Rendimiento y Escalabilidad

**Arquitectura Target**: Hybrid serverless con auto-scaling horizontal para manejar cargas variables de procesamiento LLM y vector embeddings.

**Performance Goals**:
- **Response Time**: <500ms para queries simples, <2s para an√°lisis complejos multi-agent
- **Throughput**: 1000+ requests/minute con degradaci√≥n graceful
- **Availability**: 99.9% uptime con failover autom√°tico
- **Scalability**: Auto-scaling 0‚Üí1000 instancias basado en demanda

---

## üèóÔ∏è Arquitectura de Performance & Escalabilidad - Diagrama General

```mermaid
graph TB
    subgraph "Client Layer"
        WEB[Web Frontend]
        MOB[Mobile App]
        API_CLIENT[API Clients]
    end
    
    subgraph "Load Balancer & CDN"
        ALB[Application Load Balancer<br/>Route 53 + CloudFront]
        CDN[CDN Cache<br/>Static Assets]
    end
    
    subgraph "Auto-Scaling Layer"
        ASG[Auto Scaling Groups]
        CW[CloudWatch Metrics<br/>CPU/Memory/Requests]
        ASG_POLICY[Scaling Policies<br/>Target: 70% CPU<br/>Min: 1, Max: 10]
    end
    
    subgraph "Serverless Computing"
        direction TB
        LAMBDA_ORCH[Lambda - Orchestration<br/>Optimized Cold Start<br/>Connection Pooling]
        LAMBDA_KNOW[Lambda - Knowledge<br/>Pre-warmed Models<br/>Cached Embeddings]
        LAMBDA_AUTH[Lambda - Auth<br/>JWT Management]
    end
    
    subgraph "Container Computing - EKS Cluster"
        direction TB
        EKS_LLM[EKS - LLM Processor Pod<br/>2 vCPU, 4GB RAM<br/>GPU-optimized Nodes]
        EKS_VECTOR[EKS - Vector Search Pod<br/>Optimized for pgvector<br/>HNSW Indexes]
        EKS_BATCH[EKS - Batch Processing Pod<br/>Async workflows<br/>High memory Nodes]
    end
    
    subgraph "Multi-Layer Caching"
        direction LR
        L1[Layer 1: Memory<br/>In-process cache<br/>TTL: 5min<br/>Fastest]
        L2[Layer 2: Redis<br/>Distributed cache<br/>TTL: 30min<br/>Shared]
        L3[Layer 3: Database<br/>Persistent cache<br/>TTL: 24h<br/>Fallback]
        
        L1 --> L2
        L2 --> L3
    end
    
    subgraph "Database Layer - Performance Optimized"
        direction TB
        
        subgraph "PostgreSQL Primary"
            PG_MAIN[PostgreSQL Main<br/>Connection Pool: 20<br/>Max Overflow: 50]
            PG_INDEXES[Optimized Indexes<br/>HNSW for vectors<br/>B-tree for queries]
        end
        
        subgraph "Read Replicas"
            PG_READ1[Read Replica 1<br/>Analytics queries]
            PG_READ2[Read Replica 2<br/>Vector searches]
        end
        
        subgraph "Vector Storage"
            PGVECTOR[pgvector Extension<br/>HNSW + IVFFlat indexes<br/>Domain-specific partitions]
        end
    end
    
    subgraph "Monitoring & Observability"
        direction LR
        CW_METRICS[CloudWatch<br/>Custom Metrics<br/>Error Rate<br/>Response Time<br/>Cache Hit Rate]
        ALARMS[CloudWatch Alarms<br/>High Error Rate > 5%<br/>High Latency > 2s<br/>Low Cache Hit < 70%]
        SNS[SNS Notifications<br/>Alert Management]
    end
    
    %% Client Connections
    WEB --> ALB
    MOB --> ALB
    API_CLIENT --> ALB
    
    %% Load Balancer Routing
    ALB --> LAMBDA_ORCH
    ALB --> EKS_LLM
    CDN --> WEB
    
    %% Auto-Scaling Connections
    CW --> ASG
    ASG --> ASG_POLICY
    ASG_POLICY --> EKS_LLM
    ASG_POLICY --> EKS_VECTOR
    
    %% Lambda Inter-communication
    LAMBDA_ORCH --> LAMBDA_KNOW
    LAMBDA_ORCH --> LAMBDA_AUTH
    LAMBDA_ORCH --> EKS_LLM
    
    %% EKS Pod Communication
    EKS_LLM --> EKS_VECTOR
    EKS_VECTOR --> EKS_BATCH
    
    %% Caching Flow
    LAMBDA_ORCH --> L1
    EKS_LLM --> L1
    EKS_VECTOR --> L2
    L3 --> PG_MAIN
    
    %% Database Connections
    LAMBDA_ORCH --> PG_MAIN
    EKS_LLM --> PG_MAIN
    EKS_VECTOR --> PG_READ2
    EKS_BATCH --> PG_READ1
    
    PG_MAIN --> PGVECTOR
    PG_READ1 --> PGVECTOR
    PG_READ2 --> PGVECTOR
    
    %% Monitoring Connections
    LAMBDA_ORCH --> CW_METRICS
    EKS_LLM --> CW_METRICS
    L2 --> CW_METRICS
    CW_METRICS --> ALARMS
    ALARMS --> SNS
    
    %% Styling
    classDef serverless fill:#ff9999
    classDef container fill:#99ccff
    classDef cache fill:#99ff99
    classDef database fill:#ffcc99
    classDef monitoring fill:#cc99ff
    
    class LAMBDA_ORCH,LAMBDA_KNOW,LAMBDA_AUTH serverless
    class EKS_LLM,EKS_VECTOR,EKS_BATCH container
    class L1,L2,L3 cache
    class PG_MAIN,PG_READ1,PG_READ2,PGVECTOR database
    class CW_METRICS,ALARMS,SNS monitoring
```

## Flujo de Escalabilidad Auto-Adaptativo

```mermaid
sequenceDiagram
    participant Client
    participant ALB as Load Balancer
    participant Lambda as Lambda Functions
    participant ECS as ECS Services
    participant Cache as Multi-Layer Cache
    participant DB as PostgreSQL
    participant CW as CloudWatch
    participant ASG as Auto Scaling
    
    Note over Client,ASG: Flujo Normal - Baja Carga
    Client->>ALB: Request (100 req/min)
    ALB->>Lambda: Route to Lambda
    Lambda->>Cache: Check L1 (Memory)
    Cache-->>Lambda: Cache Hit
    Lambda-->>Client: Response <500ms
    
    Note over Client,ASG: Escalamiento - Media Carga
    Client->>ALB: Request (500 req/min)
    ALB->>Lambda: Increased Load
    Lambda->>Cache: Check L1 (Miss)
    Cache->>Cache: Check L2 (Redis Hit)
    Cache-->>Lambda: Cached Result
    Lambda-->>Client: Response <800ms
    
    Lambda->>CW: Send Metrics (CPU: 60%)
    CW->>ASG: Evaluate Scaling Policy
    Note over ASG: CPU < 70% threshold<br/>No scaling needed
    
    Note over Client,ASG: Alto Tr√°fico - Escalamiento Activo
    Client->>ALB: Request (1500 req/min)
    ALB->>EKS: Route to EKS (Lambda at capacity)
    EKS->>Cache: Check Multi-Layer
    Cache->>DB: Cache Miss - Query DB
    DB-->>Cache: Results + Store L2/L3
    Cache-->>EKS: Processed Results
    EKS-->>Client: Response <2s
    
    EKS->>CW: Send Metrics (CPU: 75%, Memory: 80%)
    CW->>ASG: Trigger Scaling Policy
    ASG->>EKS: Scale Out (+2 Pods)
    Note over EKS: New pods starting<br/>Load distributed via HPA
    
    Note over Client,ASG: Pico de Tr√°fico - M√°ximo Scaling
    Client->>ALB: Request (3000 req/min)
    ALB->>EKS: Distribute across 6 pods
    
    loop Every 5 minutes
        EKS->>CW: Metrics (CPU: 85%, Errors: 2%)
        CW->>ASG: Evaluate scaling needs
        alt CPU > 70% AND Error Rate < 5%
            ASG->>EKS: Scale Out (+2 Pods/Nodes)
        else Error Rate > 5%
            ASG->>ASG: Trigger Alarm
            ASG->>CW: Send Alert
        end
    end
    
    Note over Client,ASG: Normalizaci√≥n - Scale Down
    Client->>ALB: Request (800 req/min)
    EKS->>CW: Metrics (CPU: 40%, stable 10min)
    CW->>ASG: Evaluate Scale Down
    ASG->>EKS: Scale In (-4 Pods)
    Note over EKS: Graceful shutdown<br/>Drain connections
```

## Optimizaci√≥n de Caching Multi-Capa

```mermaid
flowchart TD
    REQUEST[Incoming Request] --> CACHE_CHECK{Check Cache Layer}
    
    subgraph "Layer 1 - Memory Cache"
        L1_CHECK{Memory<br/>Hit?}
        L1_RETURN[Return Result<br/>~1ms]
        L1_STATS[Update Stats<br/>Memory Hit++]
    end
    
    subgraph "Layer 2 - Redis Cache"  
        L2_CHECK{Redis<br/>Hit?}
        L2_RETURN[Return + Backfill L1<br/>~5ms]
        L2_STATS[Update Stats<br/>Redis Hit++]
    end
    
    subgraph "Layer 3 - Database Cache"
        L3_CHECK{DB Cache<br/>Hit?}
        L3_RETURN[Return + Backfill L1,L2<br/>~20ms]
        L3_STATS[Update Stats<br/>DB Hit++]
    end
    
    subgraph "Source Data"
        DB_QUERY[Execute Query<br/>~100ms]
        COMPUTE[Process Result<br/>~200ms]
        CACHE_POPULATE[Populate All Layers<br/>Write-Through]
    end
    
    CACHE_CHECK --> L1_CHECK
    
    L1_CHECK -->|Hit| L1_RETURN
    L1_CHECK -->|Miss| L2_CHECK
    L1_RETURN --> L1_STATS
    
    L2_CHECK -->|Hit| L2_RETURN
    L2_CHECK -->|Miss| L3_CHECK  
    L2_RETURN --> L2_STATS
    
    L3_CHECK -->|Hit| L3_RETURN
    L3_CHECK -->|Miss| DB_QUERY
    L3_RETURN --> L3_STATS
    
    DB_QUERY --> COMPUTE
    COMPUTE --> CACHE_POPULATE
    CACHE_POPULATE --> L1_RETURN
    
    subgraph "Cache Management"
        TTL_CLEANUP[TTL Cleanup<br/>Every 5 minutes]
        INVALIDATION[Smart Invalidation<br/>On data changes]
        STATS_MONITOR[Monitor Hit Rates<br/>Optimize TTL values]
    end
    
    L1_STATS --> STATS_MONITOR
    L2_STATS --> STATS_MONITOR  
    L3_STATS --> STATS_MONITOR
    
    STATS_MONITOR --> TTL_CLEANUP
    STATS_MONITOR --> INVALIDATION
    
    classDef layer1 fill:#e1f5fe
    classDef layer2 fill:#f3e5f5  
    classDef layer3 fill:#e8f5e8
    classDef source fill:#fff3e0
    classDef mgmt fill:#fce4ec
    
    class L1_CHECK,L1_RETURN,L1_STATS layer1
    class L2_CHECK,L2_RETURN,L2_STATS layer2
    class L3_CHECK,L3_RETURN,L3_STATS layer3
    class DB_QUERY,COMPUTE,CACHE_POPULATE source
    class TTL_CLEANUP,INVALIDATION,STATS_MONITOR mgmt
```

---

## Explicaci√≥n de Componentes de Performance & Scalability

### üìã **Resumen de Apartados**

| Componente | Descripci√≥n | Prop√≥sito |
|------------|-------------|-----------|
| **Lambda Function Optimization** | Optimizaci√≥n de funciones serverless AWS Lambda para reducir cold starts y mejorar performance | Respuesta r√°pida (<500ms) para queries simples con costo eficiente |
| **EKS Cluster for Intensive Workloads** | Pods Kubernetes auto-escalables para procesamiento pesado de LLM y an√°lisis complejos | Manejar workloads que requieren GPU, alta memoria o procesamiento prolongado |
| **Connection Pooling Strategy** | Gesti√≥n inteligente de conexiones a base de datos con pools optimizados por ambiente | Maximizar throughput de DB y evitar exhaustion de conexiones |
| **Vector Search Optimization** | Optimizaci√≥n espec√≠fica para b√∫squedas de embeddings usando pgvector e √≠ndices HNSW | B√∫squedas de similaridad ultra-r√°pidas (<100ms) en knowledge base |
| **Multi-Layer Caching Architecture** | Sistema de cache de 3 niveles con fallback autom√°tico | Reducir latencia y carga en DB mediante cache inteligente |
| **Application Auto Scaling** | Auto-scaling basado en m√©tricas (CPU, memoria, request rate) con CloudWatch | Escalar autom√°ticamente de 0 a 1000+ instancias seg√∫n demanda |

### üîß **Detalles por Componente**

#### **Lambda Function Optimization**
- **Qu√© es**: T√©cnicas para optimizar funciones serverless
- **Incluye**: Pre-warming de conexiones, cache de modelos ML, gesti√≥n de cold starts
- **Beneficio**: Reduce latencia inicial de 2-3s a <500ms para requests subsecuentes

#### **EKS Cluster for Intensive Workloads**  
- **Qu√© es**: Pods Kubernetes que se ejecutan en nodos auto-gestionados
- **Incluye**: Deployments, HPA (Horizontal Pod Autoscaler), Cluster Autoscaler, health checks
- **Beneficio**: Maneja procesamiento LLM pesado que Lambda no puede (>15min, >10GB RAM)

#### **Connection Pooling Strategy**
- **Qu√© es**: Reutilizaci√≥n inteligente de conexiones de base de datos
- **Incluye**: Pool sizes por ambiente, connection health monitoring, failover
- **Beneficio**: Soporte 1000+ connections concurrentes sin degradar performance

#### **Vector Search Optimization**
- **Qu√© es**: Optimizaci√≥n espec√≠fica para b√∫squedas de embeddings vectoriales
- **Incluye**: √çndices HNSW/IVFFlat, particionado por dominio, cache de queries
- **Beneficio**: B√∫squedas sub-100ms en millones de vectores de 1536 dimensiones

#### **Multi-Layer Caching Architecture**
- **Qu√© es**: Sistema de cache jer√°rquico con 3 niveles de velocidad
- **Incluye**: Memory cache (1ms), Redis (5ms), Database cache (20ms)
- **Beneficio**: 70-90% cache hit rate, reduciendo carga en DB en 80%

#### **Application Auto Scaling**
- **Qu√© es**: Escalamiento autom√°tico basado en m√©tricas de aplicaci√≥n
- **Incluye**: CloudWatch alarms, scaling policies, target tracking
- **Beneficio**: Mantiene <2s response time bajo cualquier carga, optimiza costos

---

## Serverless Architecture Strategy

### Lambda Function Optimization

```python
# src/backend/deployment/lambda/function_optimizer.py
import os
import asyncio
from functools import wraps
from typing import Dict, Any, Optional
import time
import json
import logging

logger = logging.getLogger()

class LambdaOptimizer:
    """Optimizaci√≥n espec√≠fica para AWS Lambda functions"""
    
    def __init__(self):
        self.cold_start_cache = {}
        self.connection_pool = None
        self.initialized = False
        
    async def warm_up_connections(self):
        """Pre-warm database y external service connections"""
        if not self.initialized:
            # Initialize database pool
            from nura_core.database import db_manager
            await db_manager.initialize(
                os.getenv("DATABASE_URL"),
                pool_size=5,  # Reduced for Lambda
                max_overflow=10
            )
            
            # Initialize Redis cache
            from nura_core.cache import cache_manager
            await cache_manager.initialize()
            
            # Pre-load ML models if needed
            await self._preload_models()
            
            self.initialized = True
            
    async def _preload_models(self):
        """Pre-cargar modelos ML en memoria para reducir latencia"""
        # Preload embedding models
        try:
            from transformers import AutoTokenizer, AutoModel
            model_name = "sentence-transformers/all-MiniLM-L6-v2"
            
            # Cache model in Lambda container
            if not hasattr(self, 'embedding_model'):
                self.embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.embedding_model = AutoModel.from_pretrained(model_name)
                logger.info(f"Model {model_name} preloaded successfully")
                
        except Exception as e:
            logger.warning(f"Model preload failed: {str(e)}")

# Global optimizer instance
lambda_optimizer = LambdaOptimizer()

def lambda_handler_wrapper(handler_func):
    """Decorator para optimizar Lambda handlers"""
    @wraps(handler_func)
    async def wrapper(event, context):
        start_time = time.time()
        
        try:
            # Warm up connections on first invocation
            await lambda_optimizer.warm_up_connections()
            
            # Execute handler
            result = await handler_func(event, context)
            
            # Add performance metadata
            execution_time = int((time.time() - start_time) * 1000)
            if isinstance(result, dict):
                result['_metadata'] = {
                    'execution_time_ms': execution_time,
                    'cold_start': not lambda_optimizer.initialized,
                    'function_name': context.function_name,
                    'request_id': context.aws_request_id
                }
            
            return result
            
        except Exception as e:
            logger.error(f"Lambda execution error: {str(e)}")
            return {
                'statusCode': 500,
                'body': json.dumps({
                    'error': 'Internal server error',
                    'request_id': context.aws_request_id
                })
            }
            
    return wrapper

# Agent Orchestration Lambda Handler
@lambda_handler_wrapper
async def orchestration_handler(event, context):
    """Handler optimizado para orquestaci√≥n de agentes"""
    try:
        # Parse request
        body = json.loads(event.get('body', '{}'))
        
        # Initialize plugin system
        from nura_core.kernel import KernelManager
        kernel = KernelManager()
        
        # Register core plugins (cached after first run)
        if not hasattr(lambda_optimizer, 'plugins_registered'):
            await register_core_plugins(kernel)
            lambda_optimizer.plugins_registered = True
        
        # Process query
        result = await kernel.process_query(body)
        
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'body': json.dumps(result)
        }
        
    except Exception as e:
        logger.error(f"Orchestration handler error: {str(e)}")
        raise

async def register_core_plugins(kernel: KernelManager):
    """Register plugins with optimizations for Lambda environment"""
    from plugins.agent_orchestration import AgentOrchestrationPlugin
    from plugins.knowledge_management import KnowledgeManagementPlugin
    
    # Register with reduced resource footprint
    await kernel.register_plugin("agent-orchestration", AgentOrchestrationPlugin())
    await kernel.register_plugin("knowledge-management", KnowledgeManagementPlugin())
```

### EKS Cluster for Intensive Workloads

```yaml
# deployment/eks/llm-processor-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nura-llm-processor
  namespace: nura-production
  labels:
    app: nura-llm-processor
    version: v1.0.0
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nura-llm-processor
  template:
    metadata:
      labels:
        app: nura-llm-processor
    spec:
      containers:
      - name: llm-processor
        image: nura/llm-processor:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi" 
            cpu: "2000m"
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: LOG_LEVEL
          value: "INFO"
        - name: MAX_WORKERS
          value: "4"
        - name: WORKER_TIMEOUT
          value: "300"
        envFrom:
        - secretRef:
            name: nura-secrets
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
      nodeSelector:
        node-type: compute-optimized

---
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nura-llm-processor-hpa
  namespace: nura-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nura-llm-processor
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 600  # 10 minutes
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# Cluster Autoscaler for Node Scaling
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-status
  namespace: kube-system
data:
  nodes.max: "20"
  nodes.min: "2"
  scale-down-delay-after-add: "10m"
  scale-down-unneeded-time: "10m"
```

---

## Database Performance Optimization

### Connection Pooling Strategy

```python
# src/backend/nura-core/src/database/performance_optimizer.py
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.pool import QueuePool, NullPool
from sqlalchemy.dialects.postgresql import asyncpg
import os
from typing import Dict, Any
import asyncio
import logging

logger = logging.getLogger(__name__)

class DatabasePerformanceOptimizer:
    """Optimizador de performance para conexiones de base de datos"""
    
    def __init__(self):
        self.engines: Dict[str, Any] = {}
        self.connection_health = {}
        
    async def create_optimized_engine(self, database_url: str, pool_name: str = "default"):
        """Crear engine optimizado basado en environment y uso"""
        
        # Configuraci√≥n base
        base_config = {
            "echo": os.getenv("SQL_DEBUG", "false").lower() == "true",
            "future": True,
            "pool_pre_ping": True,
            "pool_recycle": 3600,  # 1 hour
        }
        
        # Environment-specific optimizations
        environment = os.getenv("ENVIRONMENT", "development")
        
        if environment == "production":
            # Production: High concurrency, connection pooling
            config = {
                **base_config,
                "poolclass": QueuePool,
                "pool_size": 20,  # Base connections
                "max_overflow": 50,  # Additional connections under load
                "pool_timeout": 30,
                "pool_reset_on_return": "commit",
            }
            
        elif environment == "staging":
            # Staging: Medium load
            config = {
                **base_config,
                "poolclass": QueuePool,
                "pool_size": 10,
                "max_overflow": 20,
                "pool_timeout": 20,
            }
            
        elif pool_name == "lambda":
            # Lambda: Minimal connections, fast startup
            config = {
                **base_config,
                "poolclass": QueuePool,
                "pool_size": 2,  # Minimal for Lambda
                "max_overflow": 5,
                "pool_timeout": 10,
                "pool_recycle": 1800,  # 30 minutes (shorter for Lambda)
            }
            
        else:
            # Development: No pooling
            config = {
                **base_config,
                "poolclass": NullPool,
            }
        
        # Crear engine con configuraci√≥n optimizada
        engine = create_async_engine(
            database_url,
            **config,
            # PostgreSQL-specific optimizations
            connect_args={
                "server_settings": {
                    "application_name": f"nura_{pool_name}",
                    "jit": "off",  # Disable JIT for short queries
                }
            }
        )
        
        self.engines[pool_name] = engine
        logger.info(f"Database engine '{pool_name}' created with {config['poolclass'].__name__}")
        
        return engine
    
    async def get_read_replica_engine(self, read_replica_url: str):
        """Engine espec√≠fico para read replicas"""
        return await self.create_optimized_engine(
            read_replica_url, 
            pool_name="read_replica"
        )
    
    async def monitor_connection_health(self):
        """Monitor health de connection pools"""
        for pool_name, engine in self.engines.items():
            try:
                pool = engine.pool
                health_stats = {
                    "pool_name": pool_name,
                    "size": pool.size(),
                    "checked_in": pool.checkedin(),
                    "checked_out": pool.checkedout(),
                    "overflow": pool.overflow(),
                    "invalid": pool.invalid(),
                }
                
                self.connection_health[pool_name] = health_stats
                
                # Log warnings para pools con problemas
                if health_stats["checked_out"] / health_stats["size"] > 0.8:
                    logger.warning(f"High connection usage in pool {pool_name}: {health_stats}")
                    
            except Exception as e:
                logger.error(f"Error monitoring pool {pool_name}: {str(e)}")
    
    async def optimize_query_performance(self, session: AsyncSession, query_type: str):
        """Optimizaciones espec√≠ficas por tipo de query"""
        
        if query_type == "analytics":
            # Para queries anal√≠ticas: incrementar work_mem
            await session.execute("SET work_mem = '256MB'")
            
        elif query_type == "vector_search":
            # Para b√∫squedas vectoriales: optimizar parallel workers
            await session.execute("SET max_parallel_workers_per_gather = 4")
            
        elif query_type == "bulk_insert":
            # Para inserts masivos: optimizar commit frequency
            await session.execute("SET synchronous_commit = OFF")

# Global optimizer
db_performance_optimizer = DatabasePerformanceOptimizer()
```

### Vector Search Optimization

```python
# src/backend/nura-core/src/vector/search_optimizer.py
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
import logging

logger = logging.getLogger(__name__)

class VectorSearchOptimizer:
    """Optimizaci√≥n para b√∫squedas vectoriales con pgvector"""
    
    def __init__(self):
        self.index_stats = {}
        self.search_cache = {}
        
    async def create_optimized_indexes(self, session: AsyncSession):
        """Crear √≠ndices optimizados para b√∫squedas vectoriales"""
        
        # HNSW index para b√∫squedas de similaridad r√°pidas
        await session.execute(text("""
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_embeddings_hnsw_cosine 
            ON embeddings USING hnsw (embedding_vector vector_cosine_ops)
            WITH (m = 16, ef_construction = 64);
        """))
        
        # IVFFlat index para datasets grandes
        await session.execute(text("""
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_embeddings_ivfflat_cosine 
            ON embeddings USING ivfflat (embedding_vector vector_cosine_ops)
            WITH (lists = 100);
        """))
        
        # √çndices parciales por dominio para performance
        await session.execute(text("""
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_embeddings_knowledge_hnsw
            ON embeddings USING hnsw (embedding_vector vector_cosine_ops)
            WHERE domain = 'knowledge_base'
            WITH (m = 16, ef_construction = 64);
        """))
        
        await session.execute(text("""
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_embeddings_conversation_hnsw
            ON embeddings USING hnsw (embedding_vector vector_cosine_ops) 
            WHERE domain = 'conversation'
            WITH (m = 16, ef_construction = 64);
        """))
        
        logger.info("Optimized vector indexes created")
    
    async def optimized_similarity_search(
        self,
        session: AsyncSession,
        query_vector: List[float],
        domain: str,
        top_k: int = 10,
        similarity_threshold: float = 0.7,
        use_cache: bool = True
    ) -> List[Dict[str, Any]]:
        """B√∫squeda de similaridad optimizada con caching"""
        
        # Generate cache key
        import hashlib
        cache_key = hashlib.md5(
            f"{str(query_vector)[:100]}_{domain}_{top_k}_{similarity_threshold}".encode()
        ).hexdigest()
        
        # Check cache first
        if use_cache and cache_key in self.search_cache:
            logger.debug(f"Vector search cache hit for domain {domain}")
            return self.search_cache[cache_key]
        
        try:
            # Optimized query with domain filtering and similarity threshold
            query = text("""
                SELECT 
                    id,
                    content,
                    metadata,
                    1 - (embedding_vector <=> :query_vector::vector) as similarity,
                    domain,
                    created_at
                FROM embeddings
                WHERE domain = :domain
                AND 1 - (embedding_vector <=> :query_vector::vector) >= :threshold
                ORDER BY embedding_vector <=> :query_vector::vector
                LIMIT :limit;
            """)
            
            result = await session.execute(
                query,
                {
                    "query_vector": query_vector,
                    "domain": domain,
                    "threshold": similarity_threshold,
                    "limit": top_k
                }
            )
            
            results = [
                {
                    "id": row.id,
                    "content": row.content,
                    "metadata": row.metadata,
                    "similarity": float(row.similarity),
                    "domain": row.domain,
                    "created_at": row.created_at
                }
                for row in result.fetchall()
            ]
            
            # Cache results for 5 minutes
            if use_cache:
                self.search_cache[cache_key] = results
                # Simple TTL implementation
                asyncio.create_task(self._expire_cache_key(cache_key, 300))
            
            logger.debug(f"Vector search returned {len(results)} results for domain {domain}")
            return results
            
        except Exception as e:
            logger.error(f"Vector search error for domain {domain}: {str(e)}")
            return []
    
    async def batch_similarity_search(
        self,
        session: AsyncSession,
        query_vectors: List[List[float]],
        domain: str,
        top_k: int = 10
    ) -> List[List[Dict[str, Any]]]:
        """B√∫squeda en lote para m√∫ltiples vectores"""
        
        # Execute searches in parallel
        search_tasks = [
            self.optimized_similarity_search(session, vector, domain, top_k, use_cache=False)
            for vector in query_vectors
        ]
        
        results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # Handle any failed searches
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Batch search failed for vector {i}: {str(result)}")
                processed_results.append([])
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _expire_cache_key(self, cache_key: str, delay: int):
        """Remove cache key after delay"""
        await asyncio.sleep(delay)
        self.search_cache.pop(cache_key, None)
    
    async def analyze_search_performance(self, session: AsyncSession):
        """Analizar performance de b√∫squedas vectoriales"""
        
        # Check index usage
        index_usage_query = text("""
            SELECT 
                schemaname,
                tablename,
                indexname,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch
            FROM pg_stat_user_indexes 
            WHERE tablename = 'embeddings'
            ORDER BY idx_scan DESC;
        """)
        
        result = await session.execute(index_usage_query)
        index_stats = [dict(row) for row in result.fetchall()]
        
        # Check table stats
        table_stats_query = text("""
            SELECT 
                n_tup_ins,
                n_tup_upd,
                n_tup_del,
                n_live_tup,
                n_dead_tup,
                last_vacuum,
                last_autovacuum,
                last_analyze,
                last_autoanalyze
            FROM pg_stat_user_tables 
            WHERE relname = 'embeddings';
        """)
        
        result = await session.execute(table_stats_query)
        table_stats = dict(result.fetchone()) if result.rowcount > 0 else {}
        
        return {
            "index_usage": index_stats,
            "table_stats": table_stats,
            "cache_hit_rate": len(self.search_cache),
            "active_domains": list(set(
                stats.get("domain", "unknown") 
                for stats in self.search_cache.values() 
                if isinstance(stats, list) and stats
            ))
        }

# Global vector search optimizer
vector_search_optimizer = VectorSearchOptimizer()
```

---

## Caching Strategy

### Multi-Layer Caching Architecture

```python
# src/backend/nura-core/src/cache/multi_layer_cache.py
import asyncio
import json
import time
from typing import Any, Optional, Dict, Union, List
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class CacheLayer(Enum):
    """Capas de cache disponibles"""
    MEMORY = "memory"        # In-memory cache (fastest)
    REDIS = "redis"         # Distributed cache (shared)
    DATABASE = "database"   # Persistent cache tables

class CacheStrategy(Enum):
    """Estrategias de caching"""
    WRITE_THROUGH = "write_through"    # Write to cache and storage simultaneously
    WRITE_BEHIND = "write_behind"      # Write to cache first, storage async
    CACHE_ASIDE = "cache_aside"        # Manual cache management

class MultiLayerCacheManager:
    """Gestor de cache multi-capa con fallback autom√°tico"""
    
    def __init__(self):
        self.memory_cache: Dict[str, Dict[str, Any]] = {}
        self.redis_cache = None
        self.cache_stats = {
            "hits": {"memory": 0, "redis": 0, "database": 0},
            "misses": {"memory": 0, "redis": 0, "database": 0},
            "writes": {"memory": 0, "redis": 0, "database": 0}
        }
        
    async def initialize(self, redis_manager):
        """Inicializar conexiones de cache"""
        self.redis_cache = redis_manager
        logger.info("Multi-layer cache manager initialized")
        
    async def get(
        self, 
        key: str, 
        namespace: str = "default",
        max_layers: List[CacheLayer] = None
    ) -> Optional[Any]:
        """Obtener valor con fallback autom√°tico entre capas"""
        
        if max_layers is None:
            max_layers = [CacheLayer.MEMORY, CacheLayer.REDIS, CacheLayer.DATABASE]
        
        cache_key = f"{namespace}:{key}"
        
        # Layer 1: Memory cache (fastest)
        if CacheLayer.MEMORY in max_layers:
            if cache_key in self.memory_cache:
                item = self.memory_cache[cache_key]
                if not self._is_expired(item):
                    self.cache_stats["hits"]["memory"] += 1
                    logger.debug(f"Memory cache hit for {cache_key}")
                    return item["value"]
                else:
                    # Remove expired item
                    del self.memory_cache[cache_key]
                    
            self.cache_stats["misses"]["memory"] += 1
        
        # Layer 2: Redis cache (distributed)
        if CacheLayer.REDIS in max_layers and self.redis_cache:
            try:
                value = await self.redis_cache.get(namespace, key)
                if value is not None:
                    self.cache_stats["hits"]["redis"] += 1
                    logger.debug(f"Redis cache hit for {cache_key}")
                    
                    # Backfill memory cache
                    await self._set_memory_cache(cache_key, value, 300)  # 5 min TTL
                    return value
                    
            except Exception as e:
                logger.warning(f"Redis cache error for {cache_key}: {str(e)}")
                
            self.cache_stats["misses"]["redis"] += 1
        
        # Layer 3: Database cache (persistent, slowest)
        if CacheLayer.DATABASE in max_layers:
            try:
                value = await self._get_database_cache(namespace, key)
                if value is not None:
                    self.cache_stats["hits"]["database"] += 1
                    logger.debug(f"Database cache hit for {cache_key}")
                    
                    # Backfill upper layers
                    await self._set_memory_cache(cache_key, value, 300)
                    if self.redis_cache:
                        await self.redis_cache.set(namespace, key, value, 1800)  # 30 min
                    
                    return value
                    
            except Exception as e:
                logger.error(f"Database cache error for {cache_key}: {str(e)}")
                
            self.cache_stats["misses"]["database"] += 1
        
        return None
    
    async def set(
        self,
        key: str,
        value: Any,
        namespace: str = "default",
        ttl: int = 3600,
        strategy: CacheStrategy = CacheStrategy.WRITE_THROUGH,
        layers: List[CacheLayer] = None
    ) -> bool:
        """Establecer valor en m√∫ltiples capas seg√∫n estrategia"""
        
        if layers is None:
            layers = [CacheLayer.MEMORY, CacheLayer.REDIS]
        
        cache_key = f"{namespace}:{key}"
        success = True
        
        try:
            if strategy == CacheStrategy.WRITE_THROUGH:
                # Write to all layers simultaneously
                tasks = []
                
                if CacheLayer.MEMORY in layers:
                    tasks.append(self._set_memory_cache(cache_key, value, min(ttl, 1800)))
                    
                if CacheLayer.REDIS in layers and self.redis_cache:
                    tasks.append(self.redis_cache.set(namespace, key, value, ttl))
                    
                if CacheLayer.DATABASE in layers:
                    tasks.append(self._set_database_cache(namespace, key, value, ttl))
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                success = all(r is True or not isinstance(r, Exception) for r in results)
                
            elif strategy == CacheStrategy.WRITE_BEHIND:
                # Write to fast layers first, database async
                if CacheLayer.MEMORY in layers:
                    await self._set_memory_cache(cache_key, value, min(ttl, 1800))
                    
                if CacheLayer.REDIS in layers and self.redis_cache:
                    await self.redis_cache.set(namespace, key, value, ttl)
                
                # Database write in background
                if CacheLayer.DATABASE in layers:
                    asyncio.create_task(self._set_database_cache(namespace, key, value, ttl))
            
            # Update stats
            for layer in layers:
                self.cache_stats["writes"][layer.value] += 1
                
            logger.debug(f"Cache set for {cache_key} using {strategy.value} strategy")
            return success
            
        except Exception as e:
            logger.error(f"Cache set error for {cache_key}: {str(e)}")
            return False
    
    async def invalidate(
        self,
        key: str,
        namespace: str = "default",
        layers: List[CacheLayer] = None
    ) -> bool:
        """Invalidar clave en todas las capas especificadas"""
        
        if layers is None:
            layers = [CacheLayer.MEMORY, CacheLayer.REDIS, CacheLayer.DATABASE]
        
        cache_key = f"{namespace}:{key}"
        
        tasks = []
        if CacheLayer.MEMORY in layers:
            self.memory_cache.pop(cache_key, None)
            
        if CacheLayer.REDIS in layers and self.redis_cache:
            tasks.append(self.redis_cache.delete(namespace, key))
            
        if CacheLayer.DATABASE in layers:
            tasks.append(self._delete_database_cache(namespace, key))
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
        
        logger.debug(f"Cache invalidated for {cache_key}")
        return True
    
    async def _set_memory_cache(self, cache_key: str, value: Any, ttl: int):
        """Set valor en memory cache con TTL"""
        self.memory_cache[cache_key] = {
            "value": value,
            "expires_at": time.time() + ttl
        }
        
    def _is_expired(self, item: Dict[str, Any]) -> bool:
        """Verificar si item de memoria ha expirado"""
        return time.time() > item.get("expires_at", 0)
    
    async def _get_database_cache(self, namespace: str, key: str) -> Optional[Any]:
        """Obtener valor del cache persistente en database"""
        # Implementation would use cache table
        # This is a placeholder for the actual database cache logic
        pass
    
    async def _set_database_cache(self, namespace: str, key: str, value: Any, ttl: int):
        """Guardar valor en cache persistente de database"""
        # Implementation would use cache table
        # This is a placeholder for the actual database cache logic
        pass
    
    async def _delete_database_cache(self, namespace: str, key: str):
        """Eliminar valor del cache de database"""
        # Implementation would use cache table
        # This is a placeholder for the actual database cache logic
        pass
    
    async def get_cache_stats(self) -> Dict[str, Any]:
        """Obtener estad√≠sticas de rendimiento del cache"""
        total_hits = sum(self.cache_stats["hits"].values())
        total_misses = sum(self.cache_stats["misses"].values())
        total_requests = total_hits + total_misses
        
        hit_rate = (total_hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            "hit_rate_percentage": round(hit_rate, 2),
            "total_requests": total_requests,
            "hits_by_layer": self.cache_stats["hits"],
            "misses_by_layer": self.cache_stats["misses"],
            "writes_by_layer": self.cache_stats["writes"],
            "memory_cache_size": len(self.memory_cache)
        }
    
    async def cleanup_expired(self):
        """Limpiar elementos expirados del memory cache"""
        current_time = time.time()
        expired_keys = [
            key for key, item in self.memory_cache.items()
            if current_time > item.get("expires_at", 0)
        ]
        
        for key in expired_keys:
            del self.memory_cache[key]
            
        if expired_keys:
            logger.info(f"Cleaned up {len(expired_keys)} expired cache entries")

# Global multi-layer cache manager
multi_cache_manager = MultiLayerCacheManager()

# Cache decorators for common patterns
def cached_result(
    namespace: str,
    ttl: int = 3600,
    strategy: CacheStrategy = CacheStrategy.WRITE_THROUGH,
    layers: List[CacheLayer] = None
):
    """Decorator para cachear resultados de funciones autom√°ticamente"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            # Generate cache key from function name and arguments
            import hashlib
            key_data = f"{func.__name__}:{str(args)}:{str(sorted(kwargs.items()))}"
            cache_key = hashlib.md5(key_data.encode()).hexdigest()
            
            # Try to get from cache
            cached_result = await multi_cache_manager.get(cache_key, namespace, layers)
            if cached_result is not None:
                return cached_result
            
            # Execute function and cache result
            result = await func(*args, **kwargs)
            await multi_cache_manager.set(
                cache_key, result, namespace, ttl, strategy, layers
            )
            
            return result
        return wrapper
    return decorator
```

---

## Auto-Scaling Configuration

### Application Auto Scaling

```python
# src/backend/deployment/autoscaling/scaling_manager.py
import boto3
import asyncio
import logging
from typing import Dict, Any, List
from datetime import datetime, timedelta
import json

logger = logging.getLogger(__name__)

class AutoScalingManager:
    """Gestor de auto-scaling para servicios Nura"""
    
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
        self.application_autoscaling = boto3.client('application-autoscaling')
        self.ecs = boto3.client('ecs')
        self.lambda_client = boto3.client('lambda')
        
    async def configure_ecs_autoscaling(
        self,
        service_name: str,
        cluster_name: str,
        min_capacity: int = 1,
        max_capacity: int = 10
    ):
        """Configurar auto-scaling para servicios ECS"""
        
        # Register scalable target
        resource_id = f"service/{cluster_name}/{service_name}"
        
        try:
            self.application_autoscaling.register_scalable_target(
                ServiceNamespace='ecs',
                ResourceId=resource_id,
                ScalableDimension='ecs:service:DesiredCount',
                MinCapacity=min_capacity,
                MaxCapacity=max_capacity
            )
            
            # CPU-based scaling policy
            cpu_policy_response = self.application_autoscaling.put_scaling_policy(
                PolicyName=f'{service_name}-cpu-scaling',
                ServiceNamespace='ecs',
                ResourceId=resource_id,
                ScalableDimension='ecs:service:DesiredCount',
                PolicyType='TargetTrackingScaling',
                TargetTrackingScalingPolicyConfiguration={
                    'TargetValue': 70.0,
                    'PredefinedMetricSpecification': {
                        'PredefinedMetricType': 'ECSServiceAverageCPUUtilization'
                    },
                    'ScaleOutCooldown': 300,  # 5 minutes
                    'ScaleInCooldown': 600,   # 10 minutes
                }
            )
            
            # Memory-based scaling policy  
            memory_policy_response = self.application_autoscaling.put_scaling_policy(
                PolicyName=f'{service_name}-memory-scaling',
                ServiceNamespace='ecs',
                ResourceId=resource_id,
                ScalableDimension='ecs:service:DesiredCount',
                PolicyType='TargetTrackingScaling',
                TargetTrackingScalingPolicyConfiguration={
                    'TargetValue': 80.0,
                    'PredefinedMetricSpecification': {
                        'PredefinedMetricType': 'ECSServiceAverageMemoryUtilization'
                    },
                    'ScaleOutCooldown': 300,
                    'ScaleInCooldown': 600,
                }
            )
            
            # Custom metrics scaling (Request rate)
            request_policy_response = self.application_autoscaling.put_scaling_policy(
                PolicyName=f'{service_name}-request-rate-scaling',
                ServiceNamespace='ecs', 
                ResourceId=resource_id,
                ScalableDimension='ecs:service:DesiredCount',
                PolicyType='TargetTrackingScaling',
                TargetTrackingScalingPolicyConfiguration={
                    'TargetValue': 1000.0,  # 1000 requests per minute per instance
                    'CustomizedMetricSpecification': {
                        'MetricName': 'RequestsPerMinute',
                        'Namespace': 'Nura/Application',
                        'Statistic': 'Average'
                    },
                    'ScaleOutCooldown': 180,  # 3 minutes for faster scale-out
                    'ScaleInCooldown': 900,   # 15 minutes for cautious scale-in
                }
            )
            
            logger.info(f"Auto-scaling configured for {service_name}")
            return {
                "cpu_policy_arn": cpu_policy_response['PolicyARN'],
                "memory_policy_arn": memory_policy_response['PolicyARN'],
                "request_policy_arn": request_policy_response['PolicyARN']
            }
            
        except Exception as e:
            logger.error(f"Error configuring auto-scaling for {service_name}: {str(e)}")
            raise
    
    async def configure_lambda_autoscaling(self, function_names: List[str]):
        """Configurar auto-scaling para funciones Lambda"""
        
        scaling_configs = []
        
        for function_name in function_names:
            try:
                # Configure provisioned concurrency auto-scaling
                self.application_autoscaling.register_scalable_target(
                    ServiceNamespace='lambda',
                    ResourceId=f"function:{function_name}:provisioned",
                    ScalableDimension='lambda:function:ProvisionedConcurrency',
                    MinCapacity=2,   # Minimum provisioned concurrency
                    MaxCapacity=100  # Maximum provisioned concurrency
                )
                
                # Utilization-based scaling
                policy_response = self.application_autoscaling.put_scaling_policy(
                    PolicyName=f'{function_name}-utilization-scaling',
                    ServiceNamespace='lambda',
                    ResourceId=f"function:{function_name}:provisioned",
                    ScalableDimension='lambda:function:ProvisionedConcurrency',
                    PolicyType='TargetTrackingScaling',
                    TargetTrackingScalingPolicyConfiguration={
                        'TargetValue': 70.0,  # 70% utilization target
                        'PredefinedMetricSpecification': {
                            'PredefinedMetricType': 'LambdaProvisionedConcurrencyUtilization'
                        },
                        'ScaleOutCooldown': 60,   # 1 minute
                        'ScaleInCooldown': 300,   # 5 minutes
                    }
                )
                
                scaling_configs.append({
                    "function_name": function_name,
                    "policy_arn": policy_response['PolicyARN']
                })
                
                logger.info(f"Auto-scaling configured for Lambda function {function_name}")
                
            except Exception as e:
                logger.error(f"Error configuring Lambda auto-scaling for {function_name}: {str(e)}")
        
        return scaling_configs
    
    async def create_custom_metrics_alarms(self):
        """Crear alarmas CloudWatch para m√©tricas personalizadas"""
        
        alarms = []
        
        # High Error Rate Alarm
        error_alarm = self.cloudwatch.put_metric_alarm(
            AlarmName='Nura-HighErrorRate',
            ComparisonOperator='GreaterThanThreshold',
            EvaluationPeriods=2,
            MetricName='ErrorRate',
            Namespace='Nura/Application',
            Period=300,  # 5 minutes
            Statistic='Average',
            Threshold=5.0,  # 5% error rate
            ActionsEnabled=True,
            AlarmActions=[
                # SNS topic for notifications
                'arn:aws:sns:us-east-1:ACCOUNT:nura-alerts'
            ],
            AlarmDescription='High error rate detected in Nura application',
        )
        alarms.append('Nura-HighErrorRate')
        
        # High Response Time Alarm
        latency_alarm = self.cloudwatch.put_metric_alarm(
            AlarmName='Nura-HighLatency',
            ComparisonOperator='GreaterThanThreshold',
            EvaluationPeriods=2,
            MetricName='ResponseTime',
            Namespace='Nura/Application',
            Period=300,
            Statistic='Average',
            Threshold=2000.0,  # 2 seconds
            ActionsEnabled=True,
            AlarmActions=[
                'arn:aws:sns:us-east-1:ACCOUNT:nura-alerts'
            ],
            AlarmDescription='High response time detected in Nura application',
        )
        alarms.append('Nura-HighLatency')
        
        # Low Cache Hit Rate Alarm
        cache_alarm = self.cloudwatch.put_metric_alarm(
            AlarmName='Nura-LowCacheHitRate',
            ComparisonOperator='LessThanThreshold',
            EvaluationPeriods=3,
            MetricName='CacheHitRate',
            Namespace='Nura/Application',
            Period=600,  # 10 minutes
            Statistic='Average',
            Threshold=70.0,  # 70% cache hit rate
            ActionsEnabled=True,
            AlarmActions=[
                'arn:aws:sns:us-east-1:ACCOUNT:nura-alerts'
            ],
            AlarmDescription='Low cache hit rate in Nura application',
        )
        alarms.append('Nura-LowCacheHitRate')
        
        logger.info(f"Created {len(alarms)} CloudWatch alarms")
        return alarms
    
    async def get_scaling_metrics(self, service_name: str, hours: int = 24) -> Dict[str, Any]:
        """Obtener m√©tricas de scaling para an√°lisis"""
        
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=hours)
        
        metrics = {}
        
        # Get CPU utilization
        cpu_response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/ECS',
            MetricName='CPUUtilization',
            Dimensions=[
                {'Name': 'ServiceName', 'Value': service_name}
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,  # 5 minutes
            Statistics=['Average', 'Maximum']
        )
        metrics['cpu_utilization'] = cpu_response['Datapoints']
        
        # Get Memory utilization
        memory_response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/ECS',
            MetricName='MemoryUtilization',
            Dimensions=[
                {'Name': 'ServiceName', 'Value': service_name}
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Average', 'Maximum']
        )
        metrics['memory_utilization'] = memory_response['Datapoints']
        
        # Get Task count (scaling events)
        task_response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/ECS',
            MetricName='RunningTaskCount',
            Dimensions=[
                {'Name': 'ServiceName', 'Value': service_name}
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Average']
        )
        metrics['task_count'] = task_response['Datapoints']
        
        return metrics

# Global auto-scaling manager
auto_scaling_manager = AutoScalingManager()
```

Esta secci√≥n de Performance & Scalability implementa:

1. **Serverless Optimization** con Lambda function warming y ECS Fargate auto-scaling
2. **Database Performance** con connection pooling optimizado y vector search optimization
3. **Multi-Layer Caching** con fallback autom√°tico (Memory ‚Üí Redis ‚Üí Database)
4. **Auto-Scaling Intelligence** con m√∫ltiples m√©tricas (CPU, Memory, Request Rate)
5. **Performance Monitoring** con CloudWatch alarmas y m√©tricas personalizadas
6. **Vector Search Optimization** espec√≠fico para embeddings y pgvector
7. **Environment-Specific Configuration** para development, staging y production
8. **Graceful Degradation** con circuit breakers y fallback strategies

El sistema est√° dise√±ado para **escalar desde 0 a 1000+ instancias autom√°ticamente** basado en demanda real, con **sub-500ms response times** para queries simples y **degradaci√≥n graceful** bajo carga alta.

¬øContinuamos con la secci√≥n **Security Architecture**?