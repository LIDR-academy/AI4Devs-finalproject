# System Patterns

## Architecture
The system relies on a **Memory Bank** architecture where state is explicitly documented in Markdown files.

### Monorepo Structure
```text
src/
├── backend/      # FastAPI service (Python 3.11+)
├── frontend/     # React 18 + TypeScript + Vite
├── agent/        # LangGraph agent (future)
└── shared/       # Shared utilities
```

## Core Components
- **Memory Bank**: `/memory-bank/` (State storage)
- **Agent Rules**: `/.agent/rules/` (Behavior enforcement)
- **Backend API**: FastAPI with Pydantic schemas
- **Frontend**: React SPA with TypeScript strict mode
- **Storage**: Supabase Storage for file uploads

## API Contract Patterns

### Frontend-Backend Interface Alignment
**Critical Pattern**: TypeScript interfaces MUST match Pydantic schemas exactly to avoid runtime errors.

#### Upload Flow Contract (T-002-BACK ↔ T-003-FRONT)

**Backend Schema** (`src/backend/schemas.py`):
```python
class UploadRequest(BaseModel):
    filename: str
    size: int
    checksum: Optional[str]

class UploadResponse(BaseModel):
    upload_url: str
    file_id: str      # ← UUID generated by backend
    filename: str     # ← Echo of original filename
```

**Frontend Interface** (`src/frontend/src/types/upload.ts`):
```typescript
interface PresignedUrlRequest {
  filename: string;
  size: number;
  checksum?: string;
}

interface PresignedUrlResponse {
  upload_url: string;
  file_id: string;    // ← Matches backend
  filename: string;   // ← Matches backend
}
```

**Historical Note**: Initial implementation used `file_key` instead of `file_id`, which caused test failures. Fixed in prompt #040 to align with backend reality.

#### Validation Report Contract (T-020-DB ↔ T-023-TEST)

**Backend Schema** (`src/backend/schemas.py`):
```python
class ValidationErrorItem(BaseModel):
    category: str
    target: Optional[str]
    message: str

class ValidationReport(BaseModel):
    is_valid: bool
    errors: List[ValidationErrorItem]
    metadata: Dict[str, Any]
    validated_at: Optional[datetime]
    validated_by: Optional[str]
```

**Frontend Interface** (`src/frontend/src/types/validation.ts`):
```typescript
interface ValidationErrorItem {
  category: string;
  target?: string;
  message: string;
}

interface ValidationReport {
  is_valid: boolean;
  errors: ValidationErrorItem[];
  metadata: Record<string, any>;
  validated_at?: string;  // ISO datetime string
  validated_by?: string;
}
```

**Usage Context**: This contract supports async validation of .3dm files by "The Librarian" agent (T-024-AGENT). `ValidationReport` is stored as JSONB in `blocks.validation_report` column (T-020-DB) and displayed in frontend validation UI (T-032-FRONT).

**Design Decisions**:
- `category`: Error types ("nomenclature", "geometry", "io") for grouping/filtering
- `target`: Optional identifier (layer name, object ID) for error localization
- `metadata`: Flexible schema for extracted Rhino user strings and layer info
- `validated_at`: Timestamp for audit trail and debugging
- `validated_by`: Worker identifier for troubleshooting distributed validation

### 3D Dashboard Schema (T-0503-DB)

**Database Schema** (`blocks` table extensions):
```sql
-- Columns for 3D rendering support
low_poly_url TEXT NULL  -- Supabase Storage URL to simplified GLB (~1000 triangles)
bbox JSONB NULL         -- 3D bounding box: {"min": [x,y,z], "max": [x,y,z]}

-- Indexes for dashboard queries
CREATE INDEX idx_blocks_canvas_query 
  ON blocks(status, tipologia, workshop_id) WHERE is_archived = false;
  
CREATE INDEX idx_blocks_low_poly_processing
  ON blocks(status) WHERE low_poly_url IS NULL AND is_archived = false;
```

**Usage Context**: These columns support the 3D Dashboard (US-005) where:
- `low_poly_url`: Generated by T-0502-AGENT after .3dm processing, consumed by T-0501-BACK endpoint
- `bbox`: Extracted from Rhino model metadata, used for camera auto-centering and spatial queries
- `idx_blocks_canvas_query`: Optimizes GET /api/parts with filters (target <500ms for 500 rows)
- `idx_blocks_low_poly_processing`: Optimizes GLB generation queue (target <10ms)

**Performance Targets**:
- Canvas query: <500ms (composite index on frequently filtered columns)
- Processing queue: <10ms (partial index on unprocessed blocks only)
- Index size: <100KB combined (validated: 24KB actual)

**Design Decision**: No pagination on `/api/parts` endpoint — canvas needs all parts loaded at once for proper 3D scene rendering. Response size kept <200KB via field selection (no heavy JSON payloads).

## Backend Architecture Patterns

### Clean Architecture (Implemented in T-004-BACK)
**Pattern**: Separation of Concerns with three-layer architecture.

**Structure**:
```text
src/backend/
├── api/              # API Layer (Controllers)
│   └── upload.py     # Endpoints, request/response handling only
├── services/         # Business Logic Layer
│   └── upload_service.py  # Core business logic
├── constants.py      # Centralized configuration
└── schemas.py        # Pydantic models (DTOs)
```

**Responsibilities**:
- **API Layer** (`api/`): HTTP handling, validation delegation, error mapping
- **Service Layer** (`services/`): Business logic, orchestration, data persistence
- **Constants** (`constants.py`): All magic strings/numbers centralized

**Example** (`T-029-BACK` - Confirm Upload with Celery enqueue):
```python
# api/upload.py (thin controller — dependency injection only)
@router.post("/confirm")
async def confirm_upload(request: ConfirmUploadRequest):
    supabase = get_supabase_client()
    celery = get_celery_client()
    upload_service = UploadService(supabase, celery_client=celery)
    success, event_id, task_id, error = upload_service.confirm_upload(...)
    if not success:
        raise HTTPException(...)
    return ConfirmUploadResponse(event_id=event_id, task_id=task_id)

# services/upload_service.py (business logic)
class UploadService:
    def confirm_upload(self, file_id, file_key):
        # 1. Verify file in storage
        # 2. Create event record
        # 3. Create block record (PENDING)
        # 4. Enqueue validation task via Celery
        # Return (success, event_id, task_id, error)
        ...
```

**Benefits**:
- Testable: Services can be unit tested without HTTP layer
- Reusable: Business logic accessible from CLI/workers/API
- Maintainable: Changes to business rules don't affect routing

### Constants Centralization
**Pattern**: All hardcoded values in `src/backend/constants.py`.

```python
# constants.py
STORAGE_BUCKET_RAW_UPLOADS = "raw-uploads"
EVENT_TYPE_UPLOAD_CONFIRMED = "upload.confirmed"
TABLE_EVENTS = "events"
ALLOWED_EXTENSION = ".3dm"
MAX_FILE_SIZE_MB = 500
```

**Enforcement**: Router and services MUST import from constants, never hardcode strings.

### Infrastructure Singletons (T-029-BACK)
**Pattern**: Singleton factory functions for shared infrastructure clients.

**Implementations**:
- `infra/supabase_client.py` → `get_supabase_client()` — Supabase REST API
- `infra/celery_client.py` → `get_celery_client()` — Celery task dispatch (backend → agent)

```python
# infra/celery_client.py (send-only client, no task execution)
_celery_client = None

def get_celery_client() -> Celery:
    global _celery_client
    if _celery_client is None:
        broker_url = os.getenv("CELERY_BROKER_URL", "redis://redis:6379/0")
        _celery_client = Celery("sf-pm-backend", broker=broker_url)
        _celery_client.conf.update(task_serializer="json", ...)
    return _celery_client
```

**Key distinction**: Backend's celery_client only sends tasks via `send_task()`. Agent's `celery_app.py` runs tasks via `@celery_app.task` decorators. They share the same broker URL but serve different roles.

### Testing Patterns
- **Backend**: pytest with integration tests for Supabase Storage
- **Frontend**: Vitest + @testing-library/react
  - **Docker Environment**: node:20-bookworm (Debian) required for jsdom stability
  - **Issue**: Alpine Linux (musl) causes fatal JavaScript memory errors with jsdom
  - **Solution**: Use glibc-based images (Debian/Ubuntu) for frontend testing

### Three.js / WebGL jsdom Mock Pattern (T-0500-INFRA)
**Problem**: `@react-three/fiber` Canvas requires a real WebGL context. jsdom has no WebGL → tests crash at collection time.

**Solution**: Global `vi.mock()` in `src/test/setup.ts` (applied before every test file):

```typescript
// setup.ts
vi.mock('@react-three/fiber', () => ({
  Canvas: ({ children }) =>
    React.createElement('div', { 'data-testid': 'three-canvas' }, children),
  useFrame: vi.fn(),
  useThree: vi.fn(() => ({ camera: {}, scene: {}, gl: {} })),
}));

vi.mock('@react-three/drei', () => ({
  useGLTF: vi.fn(() => ({ scene: {}, nodes: {}, materials: {} })),
  OrbitControls: vi.fn(() => null),
  Html: vi.fn(({ children }) => React.createElement('div', null, children)),
}));
```

**Rules**:
- All Three.js mocks live in `setup.ts` only — never in individual test files
- Canvas mock always uses `data-testid="three-canvas"` for queryability
- useGLTF mock always returns `{ scene, nodes, materials }` shape
- `vi.mock()` is automatically hoisted — import order doesn't matter
- **Docker tip**: Use `docker exec <running-container>` for tests; `--rm` containers lose node_modules

## Frontend Architecture Patterns

### Dependency Injection Pattern for Services (T-031-FRONT)
**Pattern**: Constructor injection for testable, decoupled service clients.

**Problem**: Environment variables can't be stubbed with `vi.stubEnv()` in Vitest ESM without issues. Mock factories create complexity. Tests need clean state between runs.

**Solution**: Accept optional config parameter in singleton factory.

**Implementation** (`src/frontend/src/services/supabase.client.ts`):
```typescript
/**
 * SupabaseConfig for dependency injection
 */
export interface SupabaseConfig {
  url: string;
  anonKey: string;
}

let supabaseInstance: SupabaseClient | null = null;

/**
 * Get or create the Supabase client instance (singleton pattern).
 * 
 * Mode 1 (Production): Reads from import.meta.env
 * Mode 2 (Testing): Accepts explicit config
 */
export function getSupabaseClient(config?: SupabaseConfig): SupabaseClient {
  if (supabaseInstance) return supabaseInstance;
  
  const finalConfig = config || {
    url: import.meta.env.VITE_SUPABASE_URL,
    anonKey: import.meta.env.VITE_SUPABASE_ANON_KEY,
  };
  
  if (!finalConfig.url || !finalConfig.anonKey) {
    throw new Error('Missing Supabase environment variables');
  }
  
  supabaseInstance = createClient(finalConfig.url, finalConfig.anonKey);
  return supabaseInstance;
}

/**
 * Reset singleton (testing only)
 */
export function resetSupabaseClient(): void {
  supabaseInstance = null;
}
```

**Usage in Tests**:
```typescript
// supabase.client.test.ts
beforeEach(() => {
  resetSupabaseClient();
});

it('should create client with config', () => {
  const client = getSupabaseClient({
    url: 'https://test.supabase.co',
    anonKey: 'test-key'
  });
  expect(client).toBeDefined();
});
```

**Benefits**:
- **Testability**: No env var mocking needed; pass config directly
- **Isolation**: `resetSupabaseClient()` clears state between tests
- **Production Safety**: Defaults to env vars when no config provided
- **Architecture**: Reusable for SSR, Storybook, or other frameworks
- **Type Safety**: TypeScript enforces config contract

**Trade-off**: +5 lines of code for much cleaner tests.

### Constants Extraction Pattern (Frontend)
**Pattern**: Separation of all hardcoded values from components.

**Example** (`T-031-FRONT` - Notification Service):
```typescript
// Constants for toast behavior
const TOAST_AUTO_REMOVE_MS = 5000;
const TOAST_ANIMATION_MS = 300;
const TOAST_TOTAL_DISPLAY_MS = TOAST_AUTO_REMOVE_MS + TOAST_ANIMATION_MS;
const TOAST_Z_INDEX = 9999;

// Configuration dictionary
export const NOTIFICATION_CONFIG: Record<StatusTransition, NotificationConfig> = {
  processing_to_validated: {
    title: '✓ Validation Complete',
    message: 'Block {iso_code} has passed all validations',
    borderColor: '#4caf50',
  },
  // ...
};

// Helper function to create toast
function createToastElement(content: string, borderColor: string): HTMLDivElement {
  // Reusable logic separated from showStatusNotification
}

// Public API
export function showStatusNotification(transition: StatusTransition, isoCode: string): void {
  const config = NOTIFICATION_CONFIG[transition];
  const message = config.message.replace('{iso_code}', isoCode);
  const toast = createToastElement(`${config.title} — ${message}`, config.borderColor);
  document.body.appendChild(toast);
  setTimeout(() => toast.remove(), TOAST_TOTAL_DISPLAY_MS);
}
```

**Realtime Hook Constants** (`useBlockStatusListener`):
```typescript
// Realtime channel configuration constants
const REALTIME_SCHEMA = 'public';
const REALTIME_TABLE = 'blocks';
const REALTIME_EVENT = 'UPDATE';

function getChannelName(blockId: string): string {
  return `block-${blockId}`;
}

export function useBlockStatusListener(options: UseBlockStatusListenerOptions): UseBlockStatusListenerReturn {
  const channelName = getChannelName(blockId);
  const realtimeChannel = supabase.channel(channelName);
  
  realtimeChannel.on(
    'postgres_changes',
    {
      event: REALTIME_EVENT,
      schema: REALTIME_SCHEMA,
      table: REALTIME_TABLE,
      filter: `id=eq.${blockId}`,
    },
    // ...
  );
}
```

**Component Organization (T-001-FRONT, continued)**:

## Agent (Celery Worker) Architecture Patterns

### Agent Module Structure (Implemented in T-022-INFRA)
**Pattern**: Clean Architecture with constants centralization (consistent with backend/frontend).

**Structure**:
```text
src/agent/
├── celery_app.py       # Celery instance configuration
├── config.py           # Environment-based settings (Pydantic)
├── constants.py        # Centralized configuration values
├── tasks.py            # Celery task definitions
├── requirements.txt    # Dependencies
└── Dockerfile          # Multi-stage build (dev/prod)
```

**Responsibilities**:
- **celery_app.py**: Initialize Celery app, configure broker/backend, import tasks
- **config.py**: Environment variables validation (CELERY_BROKER_URL, DATABASE_URL, etc.)
- **constants.py**: Task timeouts, retry policies, task names (immutable config)
- **tasks.py**: Business logic for async tasks (@celery_app.task decorators)

### Constants Centralization (Agent Specific)
**Pattern**: All timeout values, retry policies, and task names in `src/agent/constants.py`.

**Implementation** (`T-022-INFRA` Refactor):
```python
# constants.py
CELERY_APP_NAME = "sf_pm_agent"
TASK_TIME_LIMIT_SECONDS = 600  # 10min hard kill (.3dm files up to 500MB)
TASK_SOFT_TIME_LIMIT_SECONDS = 540  # 9min warning (allows cleanup)
WORKER_PREFETCH_MULTIPLIER = 1  # One task at a time (isolate large files)
RESULT_EXPIRES_SECONDS = 3600  # 1 hour auto-cleanup
TASK_MAX_RETRIES = 3
TASK_RETRY_DELAY_SECONDS = 60  # 1min between retries
TASK_HEALTH_CHECK = "agent.tasks.health_check"  # Type-safe task names
TASK_VALIDATE_FILE = "agent.tasks.validate_file"
```

**Usage**:
```python
# celery_app.py
from constants import TASK_TIME_LIMIT_SECONDS, WORKER_PREFETCH_MULTIPLIER

celery_app.conf.update(
    task_time_limit=TASK_TIME_LIMIT_SECONDS,
    worker_prefetch_multiplier=WORKER_PREFETCH_MULTIPLIER,
)

# tasks.py
from constants import TASK_HEALTH_CHECK, TASK_MAX_RETRIES, TASK_RETRY_DELAY_SECONDS

@celery_app.task(
    name=TASK_HEALTH_CHECK,
    max_retries=TASK_MAX_RETRIES,
    default_retry_delay=TASK_RETRY_DELAY_SECONDS
)
def health_check(self):
    ...
```

**Benefits**:
- **Consistency**: Same pattern as backend/frontend (team familiarity)
- **Maintainability**: Timeout adjustments in one place
- **Type Safety**: Task names as constants (refactoring support)
- **Testing**: Constants importable in tests for validation

**Conditional Imports Pattern**:
Agent modules support both direct execution (worker) and module imports (tests):
```python
# Support both /app execution and src.agent imports
try:
    import constants
    if hasattr(constants, 'CELERY_APP_NAME'):
        from constants import TASK_HEALTH_CHECK
    else:
        raise ImportError("Wrong constants module")  # Avoid backend/constants.py collision
except (ImportError, ModuleNotFoundError):
    from src.agent.constants import TASK_HEALTH_CHECK
```

## Folder Structure
```text
/memory-bank/   -> Documentation root
/.agent/rules/  -> Rules for AI agents
/src/           -> Source code (monorepo)
/docs/          -> Product documentation
/infra/         -> Infrastructure as code
/tests/         -> Integration tests
```

---

## User String Extraction Pattern (T-025-AGENT)

### Overview
Rhino .3dm files support embedding custom metadata as "User Strings" (key-value pairs) at three levels:
1. **Document-level**: Project metadata (e.g., `ProjectID`, `BIM_Manager`)
2. **Layer-level**: Manufacturing specs per layer (e.g., `Workshop`, `MaterialType`)
3. **Object-level**: Part-specific data (e.g., `ISO_Code`, `Mass`, `QA_Inspector`)

This pattern extracts and structures user strings for ISO-19650 compliance and manufacturing traceability.

### Data Model (`src/agent/models.py`)

**UserStringCollection** (Pydantic v2):
```python
from pydantic import BaseModel, ConfigDict, Field
from typing import Dict

class UserStringCollection(BaseModel):
    """Sparse dictionaries for user string metadata."""
    document: Dict[str, str] = Field(default_factory=dict)
    layers: Dict[str, Dict[str, str]] = Field(default_factory=dict)  # layer_name → strings
    objects: Dict[str, Dict[str, str]] = Field(default_factory=dict)  # object_uuid → strings
    
    model_config = ConfigDict(
        json_schema_extra={"example": {
            "document": {"ProjectID": "SF-2026", "BIM_Manager": "Pedro Cortés"},
            "layers": {"SF-C12-M-001": {"Workshop": "Granollers", "MaterialType": "UHPC"}},
            "objects": {"3f2504e0...": {"ISO_Code": "SF-C12-M-001", "Mass": "450kg"}}
        }}
    )
```

**FileProcessingResult** (updated):
```python
class FileProcessingResult(BaseModel):
    success: bool
    layers: List[LayerInfo]
    file_metadata: Dict[str, Any]
    user_strings: Optional[Dict[str, Any]] = None  # UserStringCollection as dict
    
    model_config = ConfigDict(from_attributes=True)
```

### Service Architecture

**UserStringExtractor** (`src/agent/services/user_string_extractor.py`):
- **Method**: `extract(model: File3dm) -> UserStringCollection`
- **Defensive patterns**:
  - `hasattr()` checks before accessing rhino3dm API properties
  - Per-item try-except (failures don't break entire extraction)
  - Graceful handling of `None` returns from `GetUserStrings()`
  - Sparse dictionaries (only include items with strings)
- **Logging**: Structured logs with context (`document_keys`, `layer_count`, `object_count`)

**Example Implementation**:
```python
def _extract_layer_strings(self, model) -> Dict[str, Dict[str, str]]:
    result = {}
    for layer in model.Layers:
        try:
            if not hasattr(layer, 'GetUserStrings'):
                continue
            strings_dict = layer.GetUserStrings()
            if strings_dict is None or not hasattr(strings_dict, 'Keys'):
                continue
            layer_strings = {key: strings_dict[key] for key in strings_dict.Keys}
            if layer_strings:  # Sparse: only add if has strings
                result[layer.Name] = layer_strings
        except Exception as e:
            logger.warning("layer_string_extraction_failed", layer=layer.Name, error=str(e))
            continue  # Don't break extraction for one bad layer
    return result
```

### Integration with RhinoParserService

**Location**: `src/agent/services/rhino_parser_service.py`
```python
# After extracting layers and file_metadata:
extractor = UserStringExtractor()
user_strings = extractor.extract(model)

return FileProcessingResult(
    success=True,
    layers=layers,
    file_metadata=file_metadata,
    user_strings=user_strings.model_dump()  # Convert to dict for Pydantic v2 compatibility
)
```

### Pydantic v2 Migration Note

**Issue**: Nested Pydantic models require `model_dump()` when passed to parent model constructors.
**Solution**: `UserStringCollection` is created in service, then serialized to `Dict[str, Any]` before assigning to `FileProcessingResult.user_strings`.
**Benefit**: Avoids Pydantic validation errors while maintaining type safety in service layer.

### Testing Strategy (TDD Complete)

**Unit Tests** (`tests/unit/test_user_string_extractor.py`): 8 tests
- Happy path: document, layer, object extraction
- Edge cases: empty strings, None returns, missing attributes
- Error handling: API exceptions, corrupt data

**Integration Tests** (`tests/integration/test_user_strings_e2e.py`): 3 tests
- E2E workflow: RhinoParserService → UserStringExtractor → FileProcessingResult
- Sparse object validation (only objects with strings in result)
- Empty file handling (empty dicts, not None)

**Results**: 11/11 PASS (2026-02-13), no regression in T-024-AGENT (6 passed, 4 skipped)

### rhino3dm API Quirks Documented

| API Method | Behavior | Defensive Strategy |
|------------|----------|-------------------|
| `model.Strings` | Can be missing in old .3dm versions | `hasattr()` check |
| `layer.GetUserStrings()` | Returns `None` if no strings | Explicit `None` check |
| `obj.Attributes.GetUserStrings()` | May throw `AttributeError` | Try-except wrapper |
| `NameValueDictionary.Keys` | Iterator (not list) | Convert to list/iterate directly |
| `obj.Attributes.Id` | Returns rhino3dm UUID object | Cast to `str()` for dict keys |

### Use Cases
1. **Sagrada Familia Project**: 46 user strings defined (ISO codes, materials, workshop assignments)
2. **ISO-19650 Compliance**: Document/layer metadata for audit trail
3. **Manufacturing Integration**: Object-level part numbers, QA notes, mass calculations
4. **Validation Rules**: Future nomenclature validation can reference extracted user strings

---
